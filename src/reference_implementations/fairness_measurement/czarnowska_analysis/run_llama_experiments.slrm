#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --gres=gpu:1
#SBATCH --mem=64G
#SBATCH --partition=a40
#SBATCH --qos=m
#SBATCH --job-name=llama_czarn_prompts
#SBATCH --output=%j_%x.out
#SBATCH --error=%j_%x.err

###############################################
# Usage:
#
# sbatch src/reference_implementations/fairness_measurement/czarnowska_analysis/run_llama_experiments.slrm
###############################################

# Note:
#	  ntasks: Total number of processes to use across world
#	  ntasks-per-node: How many processes each node should create

# Set NCCL options
# export NCCL_DEBUG=INFO
# NCCL backend to communicate between GPU workers is not provided in vector's cluster.
# Disable this option in slurm.
export NCCL_IB_DISABLE=1

if [[ "${SLURM_JOB_PARTITION}" == "t4v2" ]] || \
    [[ "${SLURM_JOB_PARTITION}" == "rtx6000" ]]; then
    echo export NCCL_SOCKET_IFNAME=bond0 on "${SLURM_JOB_PARTITION}"
    export NCCL_SOCKET_IFNAME=bond0
fi

RUN_NAMES=( "run_1" "run_2" "run_3" "run_4" "run_5" )
VENV_PATH=/ssd003/projects/aieng/public/prompt_engineering/
DATASET="SST5" # Should be one of SST5, SemEval, ZeroShot

echo "Python Venv Path: ${VENV_PATH}"

echo "World size: ${SLURM_NTASKS}"
echo "Number of nodes: ${SLURM_NNODES}"
NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

# Source the environment
source ${VENV_PATH}bin/activate
echo "Active Environment:"
which python

for RUN_NAME in "${RUN_NAMES[@]}";
do
    echo "Starting ${RUN_NAME} for DATASET ${DATASET}"

    OUTPUT_FILE="LLaMA_${RUN_NAME}_${DATASET}.out"

    echo "Run logging at: ${SERVER_OUTPUT_FILE}"

    nohup python -m src.reference_implementations.fairness_measurement.czarnowska_analysis.prompting_czarnowska_llama_7b \
        --run_id ${RUN_NAME} \
        --dataset ${DATASET} \
        > ${OUTPUT_FILE} 2>&1 &

    echo "Process Running"

    wait
    echo "Finished Process"
done
