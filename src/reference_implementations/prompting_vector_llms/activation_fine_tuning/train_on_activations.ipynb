{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "This notebook builds on the activations produced by the `compute_activations.ipynb` notebook. The cached activations are loaded from disk to faciliate the fine-tuning of a classification model on the sentiment analysis task. We have precomputed a set of activations in the resources folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_path = \"./resources/llama2_13b_activations/\"\n",
    "# Alternatively, you can try out the activations associated with OPT-175B, which we have precomputed as well. Note\n",
    "# that we only computed activations for the last layer of OPT. So there is no suffix to the activation pickles.\n",
    "# activations_path = \"./resources/opt_175b_activations/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an Activation Dataset which will load our activations from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, activations_path: str) -> None:\n",
    "        self._load_activations(activations_path)\n",
    "\n",
    "    def _load_activations(self, path: str) -> None:\n",
    "        with open(path, \"rb\") as handle:\n",
    "            cached_activations = pickle.load(handle)\n",
    "        self.activations = cached_activations[\"activations\"]\n",
    "        self.labels = cached_activations[\"labels\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.activations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[float], int]:\n",
    "        return self.activations[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing classification on the activations of the last (non-pad) token of the sequence, common practice for autoregressive models (e.g. OPT, Falcon, LLaMA-2). These activations have already been formed and only the last non-pad token activations have been stored. We stack these activation tensors and extract the sentiment labels associated with the input movie review that generated the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_last_token(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, List[int]]:\n",
    "    last_token_activations: List[torch.Tensor] = []\n",
    "    labels: List[int] = []\n",
    "    for activations, label in batch:\n",
    "        last_token_activations.append(activations)\n",
    "        labels.append(label)\n",
    "\n",
    "    activation_batch = torch.stack(last_token_activations)\n",
    "\n",
    "    return activation_batch, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a very small, two-layer, MLP that we will train on just 100 training samples to perform the sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Dict[str, int]) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(cfg[\"embedding_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "        self.out = nn.Linear(cfg[\"hidden_dim\"], cfg[\"label_dim\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations without Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the activations associated with a small training set of 100 samples and a test set with 300 samples. These activations were not generated using any prompts, just the raw text of the movie review. We'll just consider the activations from Layer 20 for our first comparisons here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_number_to_load = \"95\"\n",
    "\n",
    "train_dataset = ActivationDataset(os.path.join(activations_path, f\"train_activations_demo_{layer_number_to_load}.pkl\"))\n",
    "test_dataset = ActivationDataset(os.path.join(activations_path, f\"test_activations_demo_{layer_number_to_load}.pkl\"))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model: nn.Module, train_dataloader: DataLoader, test_dataloader: DataLoader, device: str\n",
    ") -> float:\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "    NUM_EPOCHS = 25\n",
    "    pbar = tqdm(range(NUM_EPOCHS))\n",
    "    for epoch_idx in pbar:\n",
    "        pbar.set_description(\"Epoch: %s\" % epoch_idx)\n",
    "        training_params = {\"Train-Loss\": 0.0, \"Test-Accuracy\": 0.0}\n",
    "        pbar.set_postfix(training_params)\n",
    "\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            activations, labels = batch\n",
    "            activations = activations.to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(activations)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_params[\"Train-Loss\"] = loss.detach().item()\n",
    "            pbar.set_postfix(training_params)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            for batch in test_dataloader:\n",
    "                activations, labels = batch\n",
    "                activations = activations.float().to(device)\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "\n",
    "                logits = model(activations)\n",
    "                predictions.extend((logits.argmax(dim=1) == labels))\n",
    "\n",
    "            accuracy = torch.stack(predictions).sum() / len(predictions)\n",
    "\n",
    "            training_params[\"Test-Accuracy\"] = accuracy.detach().item()\n",
    "            pbar.set_postfix(training_params)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden dimension is small (128) and the final dimension corresponds to our label space (positive, negative).\n",
    "\n",
    "__NOTE__: LLaMA-2 activations have a hidden dimension of 4096. On the other hand, if you're using the pre-computed activations for OPT-175B, these activations are much larger at 12,288."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:   0%|          | 0/25 [00:00<?, ?it/s, Train-Loss=0.562, Test-Accuracy=0]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 26.10it/s, Train-Loss=0.0365, Test-Accuracy=0.75] \n"
     ]
    }
   ],
   "source": [
    "model = MLP({\"embedding_dim\": 4096, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate_model(model, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations with Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the activations associated with a small training set of 100 samples and a test set with 300 samples that were generated using prompts as part of the input to the OPT model. The prompt structure can be seen in the `compute_activations.ipynb` notebook, but they incorporate few-shot examples and an instruction prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActivationDataset(\n",
    "    os.path.join(activations_path, f\"train_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    ")\n",
    "test_dataset = ActivationDataset(\n",
    "    os.path.join(activations_path, f\"test_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model. The hidden dimension is small (128) and the final dimension corresponds to our label space (positive, negative).\n",
    "\n",
    "__NOTE__: LLaMA-2 activations have a hidden dimension of 4096. On the other hand, if you're using the pre-computed activations for OPT-175B, these activations are much larger at 12,288."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 28.93it/s, Train-Loss=0.113, Test-Accuracy=0.957]\n"
     ]
    }
   ],
   "source": [
    "model = MLP({\"embedding_dim\": 4096, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate_model(model, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite an amazing result. Simply by including a few-shot prompt when computing the activations, we have __significantly__ increased the sampling efficiency of training this small classifier and induced an immense jump in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying the Layer from which Activations are Extracted\n",
    "\n",
    "Now, let's consider whether we get a significant variation in test accuracy depending on the layer we extract activations from.\n",
    "\n",
    "__NOTE__ This is only going to work for LLaMA-2. The precomputed activations from OPT-175 are only extracted from a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "test_accuracies_by_layer = {layer_number: 0.0 for layer_number in layer_numbers}\n",
    "\n",
    "for layer_number_to_load in layer_numbers:\n",
    "    # Define new model\n",
    "    model = MLP({\"embedding_dim\": 4096, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "    device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the proper dataset\n",
    "    train_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"train_activations_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    test_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"test_activations_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "\n",
    "    # Train and evaluate\n",
    "    test_accuracies_by_layer[layer_number_to_load] = train_and_evaluate_model(\n",
    "        model, train_dataloader, test_dataloader, device\n",
    "    )\n",
    "\n",
    "for layer_number in layer_numbers:\n",
    "    print(f\"Accuracy for Layer {layer_number} WITHOUT PROMPTS: {test_accuracies_by_layer[layer_number]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "test_accuracies_by_layer = {layer_number: 0.0 for layer_number in layer_numbers}\n",
    "model = MLP({\"embedding_dim\": 4096, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "for layer_number_to_load in layer_numbers:\n",
    "    # Define new model\n",
    "    model = MLP({\"embedding_dim\": 4096, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "    device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the proper dataset\n",
    "    train_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"train_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    test_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"test_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "\n",
    "    # Train and evaluate\n",
    "    test_accuracies_by_layer[layer_number_to_load] = train_and_evaluate_model(\n",
    "        model, train_dataloader, test_dataloader, device\n",
    "    )\n",
    "\n",
    "for layer_number in layer_numbers:\n",
    "    print(f\"Accuracy for Layer {layer_number} WITH PROMPTS: {test_accuracies_by_layer[layer_number]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
