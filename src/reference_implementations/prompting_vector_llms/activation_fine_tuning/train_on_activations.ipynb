{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "This notebook builds on the activations produced by the `compute_activations.ipynb` notebook. The cached activations are loaded from disk to faciliate the fine-tuning of a classification model on the sentiment analysis task. We have precomputed a set of activations in the resources folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_path = \"./resources/llama2_13b_activations/\"\n",
    "# Alternatively, you can try out the activations associated with OPT-175B, which we have precomputed as well. Note\n",
    "# that we only computed activations for the last layer of OPT. So there is no suffix to the activation pickles.\n",
    "# activations_path = \"./resources/opt_175b_activations/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an Activation Dataset which will load our activations from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, activations_path: str) -> None:\n",
    "        self._load_activations(activations_path)\n",
    "\n",
    "    def _load_activations(self, path: str) -> None:\n",
    "        with open(path, \"rb\") as handle:\n",
    "            cached_activations = pickle.load(handle)\n",
    "        self.activations = cached_activations[\"activations\"]\n",
    "        self.labels = cached_activations[\"labels\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.activations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[float], int]:\n",
    "        return self.activations[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing classification on the activations of the last (non-pad) token of the sequence, common practice for autoregressive models (e.g. OPT, Falcon, LLaMA-2). These activations have already been formed and only the last non-pad token activations have been stored. We stack these activation tensors and extract the sentiment labels associated with the input movie review that generated the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_last_token(batch: List[Tuple[torch.Tensor, int]]) -> Tuple[torch.Tensor, List[int]]:\n",
    "    last_token_activations: List[torch.Tensor] = []\n",
    "    labels: List[int] = []\n",
    "    for activations, label in batch:\n",
    "        last_token_activations.append(activations)\n",
    "        labels.append(label)\n",
    "\n",
    "    activation_batch = torch.stack(last_token_activations)\n",
    "\n",
    "    return activation_batch, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a very small, two-layer, MLP that we will train on just 100 training samples to perform the sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Dict[str, int]) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(cfg[\"embedding_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "        self.out = nn.Linear(cfg[\"hidden_dim\"], cfg[\"label_dim\"])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.linear(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations without Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the activations associated with a small training set of 100 samples and a test set with 300 samples. These activations were not generated using any prompts, just the raw text of the movie review. We'll just consider the activations from Layer 20 for our first comparisons here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_number_to_load = \"39\"\n",
    "\n",
    "train_dataset = ActivationDataset(os.path.join(activations_path, f\"train_activations_demo_{layer_number_to_load}.pkl\"))\n",
    "test_dataset = ActivationDataset(os.path.join(activations_path, f\"test_activations_demo_{layer_number_to_load}.pkl\"))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model: nn.Module, train_dataloader: DataLoader, test_dataloader: DataLoader, device: str\n",
    ") -> float:\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "    NUM_EPOCHS = 25\n",
    "    pbar = tqdm(range(NUM_EPOCHS))\n",
    "    for epoch_idx in pbar:\n",
    "        pbar.set_description(\"Epoch: %s\" % epoch_idx)\n",
    "        training_params = {\"Train-Loss\": 0.0, \"Test-Accuracy\": 0.0}\n",
    "        pbar.set_postfix(training_params)\n",
    "\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            activations, labels = batch\n",
    "            activations = activations.to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(activations)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_params[\"Train-Loss\"] = loss.detach().item()\n",
    "            pbar.set_postfix(training_params)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            for batch in test_dataloader:\n",
    "                activations, labels = batch\n",
    "                activations = activations.float().to(device)\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "\n",
    "                logits = model(activations)\n",
    "                predictions.extend((logits.argmax(dim=1) == labels))\n",
    "\n",
    "            accuracy = torch.stack(predictions).sum() / len(predictions)\n",
    "\n",
    "            training_params[\"Test-Accuracy\"] = accuracy.detach().item()\n",
    "            pbar.set_postfix(training_params)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden dimension is small (128) and the final dimension corresponds to our label space (positive, negative).\n",
    "\n",
    "__NOTE__: LLaMA-2 activations have a hidden dimension of 5120. On the other hand, if you're using the pre-computed activations for OPT-175B, these activations are much larger at 12,288."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11:  32%|███▏      | 8/25 [00:00<00:00, 37.62it/s, Train-Loss=0.0721, Test-Accuracy=0]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 38.09it/s, Train-Loss=0.0091, Test-Accuracy=0.863] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8633)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP({\"embedding_dim\": 5120, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate_model(model, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Model for Activations with Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the activations associated with a small training set of 100 samples and a test set with 300 samples that were generated using prompts as part of the input to the OPT model. The prompt structure can be seen in the `compute_activations.ipynb` notebook, but they incorporate few-shot examples and an instruction prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActivationDataset(\n",
    "    os.path.join(activations_path, f\"train_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    ")\n",
    "test_dataset = ActivationDataset(\n",
    "    os.path.join(activations_path, f\"test_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a relatively simple script to train and evaluate our model. The hidden dimension is small (128) and the final dimension corresponds to our label space (positive, negative).\n",
    "\n",
    "__NOTE__: LLaMA-2 activations have a hidden dimension of 5120. On the other hand, if you're using the pre-computed activations for OPT-175B, these activations are much larger at 12,288."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 43.31it/s, Train-Loss=0.0753, Test-Accuracy=0.887]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8867)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP({\"embedding_dim\": 5120, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate_model(model, train_dataloader, test_dataloader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite an interesting result. Simply by including a few-shot prompt when computing the activations, we have  increased the sampling efficiency of training this small classifier and induced an measurable jump in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying the Layer from which Activations are Extracted\n",
    "\n",
    "Now, let's consider whether we get a significant variation in test accuracy depending on the layer we extract activations from.\n",
    "\n",
    "__NOTE__ This is only going to work for LLaMA-2. The precomputed activations from OPT-175 are only extracted from a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 44.39it/s, Train-Loss=0.219, Test-Accuracy=0.757]\n",
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 43.79it/s, Train-Loss=0.0196, Test-Accuracy=0.857]\n",
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 43.83it/s, Train-Loss=0.00886, Test-Accuracy=0.85] \n",
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 47.95it/s, Train-Loss=0.00922, Test-Accuracy=0.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Layer 10 WITHOUT PROMPTS: 0.7566666603088379\n",
      "Accuracy for Layer 20 WITHOUT PROMPTS: 0.8566666841506958\n",
      "Accuracy for Layer 30 WITHOUT PROMPTS: 0.8500000238418579\n",
      "Accuracy for Layer 39 WITHOUT PROMPTS: 0.8500000238418579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "test_accuracies_by_layer = {layer_number: 0.0 for layer_number in layer_numbers}\n",
    "\n",
    "for layer_number_to_load in layer_numbers:\n",
    "    # Define new model\n",
    "    model = MLP({\"embedding_dim\": 5120, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "    device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the proper dataset\n",
    "    train_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"train_activations_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    test_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"test_activations_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "\n",
    "    # Train and evaluate\n",
    "    test_accuracies_by_layer[layer_number_to_load] = train_and_evaluate_model(\n",
    "        model, train_dataloader, test_dataloader, device\n",
    "    )\n",
    "\n",
    "for layer_number in layer_numbers:\n",
    "    print(f\"Accuracy for Layer {layer_number} WITHOUT PROMPTS: {test_accuracies_by_layer[layer_number]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 31.60it/s, Train-Loss=0.394, Test-Accuracy=0.75] \n",
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 47.77it/s, Train-Loss=0.0631, Test-Accuracy=0.893]\n",
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 53.32it/s, Train-Loss=0.0394, Test-Accuracy=0.873]\n",
      "Epoch: 24: 100%|██████████| 25/25 [00:00<00:00, 39.34it/s, Train-Loss=0.0713, Test-Accuracy=0.877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Layer 10 WITH PROMPTS: 0.75\n",
      "Accuracy for Layer 20 WITH PROMPTS: 0.8933333158493042\n",
      "Accuracy for Layer 30 WITH PROMPTS: 0.8733333349227905\n",
      "Accuracy for Layer 39 WITH PROMPTS: 0.8766666650772095\n"
     ]
    }
   ],
   "source": [
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "test_accuracies_by_layer = {layer_number: 0.0 for layer_number in layer_numbers}\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "for layer_number_to_load in layer_numbers:\n",
    "    # Define new model\n",
    "    model = MLP({\"embedding_dim\": 5120, \"hidden_dim\": 128, \"label_dim\": 2})\n",
    "    device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the proper dataset\n",
    "    train_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"train_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    test_dataset = ActivationDataset(\n",
    "        os.path.join(activations_path, f\"test_activations_with_prompts_demo_{layer_number_to_load}.pkl\")\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, collate_fn=batch_last_token)\n",
    "\n",
    "    # Train and evaluate\n",
    "    test_accuracies_by_layer[layer_number_to_load] = train_and_evaluate_model(\n",
    "        model, train_dataloader, test_dataloader, device\n",
    "    )\n",
    "\n",
    "for layer_number in layer_numbers:\n",
    "    print(f\"Accuracy for Layer {layer_number} WITH PROMPTS: {test_accuracies_by_layer[layer_number]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few interesting takeaways from these results:\n",
    "\n",
    "* The prompt structures we provided helped to improve the results. For the best performing layer activations we move from `0.856%` accuracy without prompts to `0.893%` with them.\n",
    "\n",
    "* In both cases, the best layer activations for the task are not the first or last layers, but rather Layer 20. This has been observed in other contexts, as the earlier layers contain more general embeddings.\n",
    "\n",
    "* This increase in performance is nice, but it is actually much lower than for OPT (you can test those activations in this notebook by switching where the activations are loaded from). While unprompted LLaMA-2 performs this task much better than OPT in the same settings, using prompts boosts OPTs task accuracy to the high 90s in terms of percent. That's a really impressive jump!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_fine_tune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
