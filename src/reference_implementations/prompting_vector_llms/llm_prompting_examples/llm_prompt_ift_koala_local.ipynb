{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nuclear-alaska",
   "metadata": {},
   "source": [
    "At a minimum, it is best to allocate at least 48GB of CPU memory to load this model. For fast inference, you'll also have to have the notebook backed by an A40 GPU.\n",
    "\n",
    "__NOTE__: You will not be able to load this model if you start the notebook through Jupyter Hub. Those notebooks are only backed by 16 GB of CPU memory and a T4V2 GPU. To start up a notebook backed by a specific GPU, follow the instructions in the top level README.MD of this repository and those in `src/reference_implementations/prompting_vector_llms/README.MD`\n",
    "\n",
    "## Prompting Instruction Fine-tuned language models\n",
    "Language Models such as Falcon and LLaMA are trained to do next-token prediction (autocompletion). As a result, they might easily miss the fact that we expect it to answer our question instead of providing a reasonable (but not very helpful) \"autocompletion\" to our prompt. One way to steer the LM away from this behavior is instruction fine-tuning.\n",
    "\n",
    "In this notebook, you will find an example of how to work with an instruction fine-tuned (IFT) LLM (namely Koala), and how they can help simplify your prompt design workflow. \n",
    "\n",
    "Overall steps for calling an IFT model can include:\n",
    "- Loading the model: either locally as a HuggingFace pipeline, or using a remote model API as in the Kaleidoscope examples;\n",
    "- Pre-processing your text query. You might need to add special tokens to your input to achieve full instruction fine-tuning potentials; And,\n",
    "- Extracting structured information from the model output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-hepatitis",
   "metadata": {},
   "source": [
    "To speed things up, we've cached weights of a number of IFT models on the Vector cluster. If you prefer, you may also download these weights directly from the HuggingFace Hub:\n",
    "- Koala-7B: https://huggingface.co/TheBloke/koala-7B-HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-metallic",
   "metadata": {},
   "source": [
    "## Koala IFT model\n",
    "Similar to Alpaca, the [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) model is an instruction fine-tuned version of Facebook AI's LLaMA LLM (This is the first version of LLaMA, not LLaMA-2). \n",
    "\n",
    "The following example is based on the 7B version of Koala (13GB at 16-bit).\n",
    "\n",
    "Note that initializing the pipeline might take a while: Up to five minutes for a 7B model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secondary-safety",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "MODEL_PATH = \"/ssd005/projects/llm/koala-7B-HF\"\n",
    "generator = pipeline(\"text-generation\", model=MODEL_PATH, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-terry",
   "metadata": {},
   "source": [
    "### Pre-process Query\n",
    "To build an IFT LLM, we fine-tune the base model (e.g., Falcon, LLaMA) on demonstrations where the LLM needs to produce outputs that follow human instructions. To remind the model about its role as the \"assistant\" and not the \"human,\" we would add special separator tokens between the human instruction and what the model needs to generate. \n",
    "\n",
    "For example, in the Koala model, the authors added the following separators between instructions and demonstrations:\n",
    "\n",
    "- Add \"BEGINNING OF CONVERSATION: \" to the beginning of each conversation,\n",
    "- Add \"USER: \" before each human input,\n",
    "- Add \"GPT: \" after the human query, and\n",
    "- Add \"\\</s\\>\" (a special token) to the end of each LM output.\n",
    "\n",
    "During inference, we would need to add the same set of tokens to make the most out of the instruction fine-tuned model. \n",
    "\n",
    "Refer to Koala's [documentation](https://github.com/young-geng/EasyLM/blob/main/docs/koala.md) for more details.\n",
    "\n",
    "Google's [blog post](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html) on FLAN provides additional detail on how Instruction Fine-Tuning can potentially improve the zero-shot performance of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_koala_input(user_input: str) -> str:\n",
    "    return \"BEGINNING OF CONVERSATION: USER: \" + user_input + \" GPT:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-minimum",
   "metadata": {},
   "source": [
    "### Run pre-processed query through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serious-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"We had a great experience at the restaurant, food was delicious, but the service was kinda bad.             \n",
    "\n",
    "What is the sentiment on the restaurant, positive or negative? Explain your reasoning.\"\"\"  # noqa: W291\n",
    "\n",
    "text_input = preprocess_koala_input(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capital-raising",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to model: \n",
      "###\n",
      "BEGINNING OF CONVERSATION: USER: We had a great experience at the restaurant, food was delicious, but the service was kinda bad.             \n",
      "\n",
      "What is the sentiment on the restaurant, positive or negative? Explain your reasoning. GPT:\n",
      "###\n",
      "Pipeline output: \n",
      "###\n",
      "[{'generated_text': 'BEGINNING OF CONVERSATION: USER: We had a great experience at the restaurant, food was delicious, but the service was kinda bad.             \\n\\nWhat is the sentiment on the restaurant, positive or negative? Explain your reasoning. GPT: Based on the information provided, it seems that the sentiment towards the restaurant is mostly positive, with the customer mentioning that the food was delicious. However, the customer also mentions that the service was not as good as they expected. This suggests that the customer had a positive experience with the food, but was disappointed with the service.\\n\\nTherefore, the sentiment towards the restaurant is mostly positive, but with a slight negative tone.'}]\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(\"Input to model: \\n###\\n\" + text_input + \"\\n###\")\n",
    "\n",
    "hf_pipeline_output = generator(text_input, max_new_tokens=128)\n",
    "\n",
    "print(\"Pipeline output: \\n###\\n\" + str(hf_pipeline_output) + \"\\n###\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-tunnel",
   "metadata": {},
   "source": [
    "### Extract useful info from model output\n",
    "There are many ways to extract structural information (e.g., binary label for sentiment: positive/label) from the natural text output of the language model. \n",
    "\n",
    "Since the focus of this notebook is on prompting instruction fine-tuned models, we will demonstrate only a basic example where we delete previous input to the model and keep only the newly-generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amino-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_koala_response(model_output: str, previous_input: str) -> str:\n",
    "    return model_output.replace(previous_input, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-therapy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted output: \n",
      "###\n",
      " Based on the information provided, it seems that the sentiment towards the restaurant is mostly positive, with the customer mentioning that the food was delicious. However, the customer also mentions that the service was not as good as they expected. This suggests that the customer had a positive experience with the food, but was disappointed with the service.\n",
      "\n",
      "Therefore, the sentiment towards the restaurant is mostly positive, but with a slight negative tone.\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "model_output = hf_pipeline_output[0][\"generated_text\"]\n",
    "extracted_output = extract_koala_response(model_output, previous_input=text_input)\n",
    "\n",
    "print(\"Extracted output: \\n###\\n\" + extracted_output + \"\\n###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47230f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
