{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=4001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"falcon-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_generation_config = {\"max_tokens\": 128, \"top_p\": 1.0, \"temperature\": 1.0}\n",
    "short_generation_config = {\"max_tokens\": 10, \"top_p\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a basic prompt for factual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ottawa is the capital of Canada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", short_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "print(generation.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_generations(generation_text: str) -> str:\n",
    "    # This simply attempts to extract the first three \"sentences\" within a generated string\n",
    "    split_text = re.findall(r\".*?[.!\\?]\", generation_text)[0:3]\n",
    "    split_text = [text.strip() for text in split_text]\n",
    "    return \" \".join(split_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Prompts\n",
    "\n",
    "Now let's create a basic prompt template that we can reuse for multiple text inputs. This will be an instruction prompt with an unconstrained answer space as we're going to try to get Falcon to summarize texts. We'll try several different templates and examine performance for each. Note that this section simply considers \"manual\" or \"human-level\" inspection to determine the quality of the summary. At the bottom of this notebook, we consider measuring the quality of two prompts on a sample of the CNN Daily Mail task using a ROUGE-1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summary_1 = \"Summarize the preceding text.\"\n",
    "prompt_template_summary_2 = \"Short Summary:\"\n",
    "prompt_template_summary_3 = \"TLDR;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resources/news_summary_datasets/examples_news.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    news_stories = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_with_template_1 = [f\"{news_story} {prompt_template_summary_1}\" for news_story in news_stories]\n",
    "prompts_with_template_2 = [f\"{news_story}\\n{prompt_template_summary_2}\" for news_story in news_stories]\n",
    "prompts_with_template_3 = [f\"{news_story}\\n{prompt_template_summary_3}\" for news_story in news_stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, we use the prompt structures\n",
    "\n",
    "* (text) Summarize the preceding text.\n",
    "* (text) \n",
    "\n",
    "  Short Summary:\n",
    "* (text) \n",
    "\n",
    "  TLDR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the preceding text.\n",
      "Original Length: 1262, Summary Length: 278\n",
      "The US believes that Russia is sending captured Western weapons to Iran to reverse-engineer. The US believes that Russia is sending captured Western weapons to Iran to reverse-engineer. The US believes that Russia is sending captured Western weapons to Iran to reverse-engineer.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the preceding text.\n",
      "Original Length: 1181, Summary Length: 599\n",
      "The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties. The National Weather Center said Thursday that the storm could bring “significant rises along streams and rivers, with widespread flooding impacts possible through early next week. The threat has pushed local officials to issue evacuation warnings and orders for some areas in the storm’s most precarious path as well as remind residents to prepare for yet another bout of severe weather.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the preceding text.\n",
      "Original Length: 1260, Summary Length: 410\n",
      "The state’s request comes as the Supreme Court is considering a case that could have a major impact on transgender rights. The justices are scheduled to hear arguments in a case involving a Virginia student who was barred from using the boys’ bathroom at his high school. The court is also considering a case involving a Colorado transgender student who was barred from using the girls’ bathroom at her school.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_1 = []\n",
    "for prompt_with_template_1, original_story in zip(prompts_with_template_1, news_stories):\n",
    "    generation_1.append(model.generate(prompt_with_template_1, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_1}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_1[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Short Summary:\n",
      "Original Length: 1262, Summary Length: 535\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons and anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Short Summary:\n",
      "Original Length: 1181, Summary Length: 526\n",
      "- California officials issue evacuation warnings in portions of several counties amid powerful storms likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state. - The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties. - The National Weather Service issued a Level 4 of 4 warning of excessive rainfall in the area.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Short Summary:\n",
      "Original Length: 1260, Summary Length: 635\n",
      "West Virginia on Thursday asked the US Supreme Court to allow it to enforce a state law that prohibits transgender women and girls from participating in public school sports. The emergency request filed to the court by state Attorney General Patrick Morrisey gives the justices a chance to weigh in on a hot-button issue that has taken center stage in recent years as Republican-led states have moved to impose restrictions on the lives of trans youth, with a particular focus on school sports. A transgender student athlete in the state quickly sued, and a district court temporarily blocked the law three months after it was enacted.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_2 = []\n",
    "for prompt_with_template_2, original_story in zip(prompts_with_template_2, news_stories):\n",
    "    generation_2.append(model.generate(prompt_with_template_2, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_2}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_2[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: TLDR;\n",
      "Original Length: 1262, Summary Length: 126\n",
      "Russia is sending captured US weapons to Iran to reverse engineer. I'm sure the US is doing the same thing. I'm sure they are.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: TLDR;\n",
      "Original Length: 1181, Summary Length: 247\n",
      "California is getting hit with another round of heavy rain and flooding. I'm in the Bay Area and we're getting hit with a lot of rain. I'm not sure if it's the same storm system as the one in the article, but it's been raining pretty much all day.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: TLDR;\n",
      "Original Length: 1260, Summary Length: 214\n",
      "West Virginia is asking the Supreme Court to allow them to enforce a law that bans trans girls from playing sports. I'm not sure how this is going to go. I'm not sure how the Supreme Court is going to rule on this.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generation_3 = []\n",
    "for prompt_with_template_3, original_story in zip(prompts_with_template_3, news_stories):\n",
    "    generation_3.append(model.generate(prompt_with_template_3, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_3}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_3[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Story 2 is about the possibility of severe flooding in California and an evacuation order being issued. Let's see what we get that from the three summaries and maybe which worked better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the preceding text.|| The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties. The National Weather Center said Thursday that the storm could bring “significant rises along streams and rivers, with widespread flooding impacts possible through early next week. The threat has pushed local officials to issue evacuation warnings and orders for some areas in the storm’s most precarious path as well as remind residents to prepare for yet another bout of severe weather.\n",
      "====================================================================================\n",
      "Short Summary:|| - California officials issue evacuation warnings in portions of several counties amid powerful storms likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state. - The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties. - The National Weather Service issued a Level 4 of 4 warning of excessive rainfall in the area.\n",
      "====================================================================================\n",
      "TLDR;|| California is getting hit with another round of heavy rain and flooding. I'm in the Bay Area and we're getting hit with a lot of rain. I'm not sure if it's the same storm system as the one in the article, but it's been raining pretty much all day.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{prompt_template_summary_1}|| {post_process_generations(generation_1[1].generation['sequences'][0])}\")\n",
    "print(\"====================================================================================\")\n",
    "print(f\"{prompt_template_summary_2}|| {post_process_generations(generation_2[1].generation['sequences'][0])}\")\n",
    "print(\"====================================================================================\")\n",
    "print(f\"{prompt_template_summary_3}|| {post_process_generations(generation_3[1].generation['sequences'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we improve the results by providing additional context to our instructions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we prompt the model to provide a summary and try to do so in a compact way. We still post-process the text to grab the first three sentences, but hopefully the model tries to pack more information into those first sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1262, Summary Length: 508\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems. The US doesn’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces. Still, US officials acknowledge that the issue is difficult to track.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1181, Summary Length: 549\n",
      "California officials issued evacuation warnings in portions of several counties amid powerful storms likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state Friday. The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties, according to the Weather Prediction Center, which issued a Level 4 of 4 warning of excessive rainfall in the area.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Summarize the text in as few words as possible:\n",
      "Original Length: 1260, Summary Length: 593\n",
      "The West Virginia Attorney General’s Office has asked the US Supreme Court to allow the state to enforce a law that prohibits transgender women and girls from participating in public school sports. The West Virginia Attorney General’s Office has asked the US Supreme Court to allow the state to enforce a law that prohibits transgender women and girls from participating in public school sports. The West Virginia Attorney General’s Office has asked the US Supreme Court to allow the state to enforce a law that prohibits transgender women and girls from participating in public school sports.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_4 = \"Summarize the text in as few words as possible:\"\n",
    "prompts_with_template_4 = [f\"{news_story} {prompt_template_summary_4}\" for news_story in news_stories]\n",
    "\n",
    "generation_4 = []\n",
    "for prompt_with_template_4, original_story in zip(prompts_with_template_4, news_stories):\n",
    "    generation_4.append(model.generate(prompt_with_template_4, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_4}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_4[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some generative models have been reported to perform better when not prompted with \"declarative\" instructions or direct interrogatives (See the [OPT Paper](https://arxiv.org/abs/2205.01068)). As such, let's ask for the summary as a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How would you briefly summarize the text?\n",
      "Original Length: 1262, Summary Length: 416\n",
      "The text is a very important document. It is a very important document because it is the first time that the United States and the European Union have agreed on a common position on the issue of the Russian invasion of Ukraine. It is a very important document because it is the first time that the United States and the European Union have agreed on a common position on the issue of the Russian invasion of Ukraine.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: How would you briefly summarize the text?\n",
      "Original Length: 1181, Summary Length: 151\n",
      "The text is about the California’s weather. What is the main idea of the text? The main idea of the text is that California is facing a severe weather.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: How would you briefly summarize the text?\n",
      "Original Length: 1260, Summary Length: 264\n",
      "The text of the law is very straightforward. It says that a student who is a biological male cannot participate in a girls’ sport or a girls’ athletic activity. It also says that a biological female cannot participate in a boys’ sport or a boys’ athletic activity.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_5 = \"How would you briefly summarize the text?\"\n",
    "prompts_with_template_5 = [f\"{news_story} {prompt_template_summary_5}\" for news_story in news_stories]\n",
    "\n",
    "generation_5 = []\n",
    "for prompt_with_template_5, original_story in zip(prompts_with_template_5, news_stories):\n",
    "    generation_5.append(model.generate(prompt_with_template_5, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_5}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_5[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summaries generated above are quite poor. Rephrasing the question will likely induce different summarization and possibly improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1262, Summary Length: 488\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. What is the significance of this story? The US and its allies have been providing Ukraine with a wide range of weapons and equipment since the beginning of the war, including Javelin anti-tank and Stinger anti-aircraft systems.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1181, Summary Length: 157\n",
      "The story is about the California weather. What is the main point of the story? The main point of the story is that California is experiencing a lot of rain.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: Briefly, what is this story about?\n",
      "Original Length: 1260, Summary Length: 327\n",
      "West Virginia is asking the US Supreme Court to allow it to enforce a state law that prohibits transgender women and girls from participating in public school sports. Why is this story important? The Supreme Court has not yet ruled on the constitutionality of laws that restrict trans youth from participating in school sports.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_6 = \"Briefly, what is this story about?\"\n",
    "prompts_with_template_6 = [f\"{news_story} {prompt_template_summary_6}\" for news_story in news_stories]\n",
    "\n",
    "generation_6 = []\n",
    "for prompt_with_template_6, original_story in zip(prompts_with_template_6, news_stories):\n",
    "    generation_6.append(model.generate(prompt_with_template_6, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_6}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_6[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, rather than asking a question, we putting the task in a context that might be more natural for a generative model. That is, we ask it to \"sum up\" the article with a natural phrase prefix to be completed in a \"conversational\" way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In short,\n",
      "Original Length: 1262, Summary Length: 416\n",
      "the US is worried that Russia is giving Iran the blueprints for the weapons it has captured. The US has been providing Ukraine with weapons since the beginning of the war, and the US has been providing Ukraine with weapons since the beginning of the war. The US has been providing Ukraine with weapons since the beginning of the war, and the US has been providing Ukraine with weapons since the beginning of the war.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: In short,\n",
      "Original Length: 1181, Summary Length: 488\n",
      "the storm is expected to bring “significant” rainfall to the region, with some areas receiving up to 10 inches of rain, according to the National Weather Service. The storm is expected to bring “significant” rainfall to the region, with some areas receiving up to 10 inches of rain, according to the National Weather Service. The storm is expected to bring “significant” rainfall to the region, with some areas receiving up to 10 inches of rain, according to the National Weather Service.\n",
      "====================================================================================\n",
      "\n",
      "Prompt: In short,\n",
      "Original Length: 1260, Summary Length: 255\n",
      "the law is a “common-sense measure” that “protects the privacy and safety of female student athletes,” the attorneys wrote. “The Act is not a ban on transgender athletes. It simply requires that they compete in sports that align with their biological sex.\n",
      "====================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_7 = \"In short,\"\n",
    "prompts_with_template_7 = [f\"{news_story} {prompt_template_summary_7}\" for news_story in news_stories]\n",
    "\n",
    "generation_7 = []\n",
    "for prompt_with_template_7, original_story in zip(prompts_with_template_7, news_stories):\n",
    "    generation_7.append(model.generate(prompt_with_template_7, long_generation_config))\n",
    "    print(f\"Prompt: {prompt_template_summary_7}\")\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summary = post_process_generations(generation_7[-1].generation[\"sequences\"][0])\n",
    "    print(f\"Original Length: {len(original_story)}, Summary Length: {len(summary)}\")\n",
    "    print(summary)\n",
    "    print(\"====================================================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Performance on CNN Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0d4e5046be484b83d145debb67519d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de808e245fe419c8c5d08725fab40ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efa6a5bf0e74e418cab6f819de05bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe44ad93e2c430dbcfce92aaabfaa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c781ed87a94411988cafddfcacebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cd950ca73b48639884ec2ad7b0c101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab9efb86b9b47a8a80d2250c56860d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2893fe152c94ec6a6cf99d741913954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8e06f2840c4d26841a3bd3ea2c222b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the CNN Daily Mail Test set, the ROUGE metric scorer from Hugging Face, and a Tokenizer from Falcon. The tokenizer is used to truncate the text such that it fits nicely into the Falcon model context. We truncate the text to 1023, so that it is of length 1024 when the start-of-sentence token (`<s>`) is added.\n",
    "\n",
    "__NOTE__: All Falcon models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705d71b226c041caa0126090ea65f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aab35bd9884fe2b82abc561c88726c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a955f5850aa433e95ed71d3f541736a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dacf8f91e2b4647ab2ee931dcf41b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94969c4dd5c346deaacc4c7fd36416f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "falcon_tokenizer = AutoTokenizer.from_pretrained(\"Rocketknight1/falcon-rw-1b\")\n",
    "dataloader = DataLoader(dataset[\"test\"], shuffle=False, batch_size=10)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "prompt_template_summary_1 = \"Briefly, what is this story about?\"\n",
    "prompt_template_summary_2 = \"Summary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_article_text(article_text: str, tokenizer: AutoTokenizer, max_sequence_length: int = 1023) -> str:\n",
    "    tokenized_article = tokenizer.encode(article_text, truncation=True, max_length=max_sequence_length)\n",
    "    return tokenizer.decode(tokenized_article, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try two different prompts from the examples above and consider how well they each do in terms of rouge score against reference summaries on the CNN Daily Mail task, which is a common summarization benchmark. You can see a discussion of this dataset here: [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the First prompt structure\n",
    "\n",
    "(text) Briefly, what is this story about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch: 1\n",
      "Processing Batch: 2\n",
      "Processing Batch: 3\n",
      "Processing Batch: 4\n",
      "Processing Batch: 5\n",
      "Processing Batch: 6\n",
      "Processing Batch: 7\n",
      "Processing Batch: 8\n",
      "Processing Batch: 9\n",
      "Processing Batch: 10\n",
      "Final Rouge Score: 0.2645435824040951\n"
     ]
    }
   ],
   "source": [
    "# Running the first prompt type\n",
    "max_batches = 10\n",
    "batch_rouge_scores = []\n",
    "for batch_number, batch in enumerate(dataloader, 1):\n",
    "    if batch_number > max_batches:\n",
    "        break\n",
    "    print(f\"Processing Batch: {batch_number}\")\n",
    "    truncated_articles = [truncate_article_text(text, falcon_tokenizer) for text in batch[\"article\"]]\n",
    "    prompts = [f\"{article_text} {prompt_template_summary_1}\" for article_text in truncated_articles]\n",
    "\n",
    "    summaries = []\n",
    "    for p in prompts:\n",
    "        summaries.append(model.generate(p, long_generation_config).generation[\"sequences\"][0])\n",
    "\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summaries = [post_process_generations(summary) for summary in summaries]\n",
    "    # References for the metric need to be in the form of list of lists\n",
    "    # (ROUGE can admit multiple references per prediction)\n",
    "    highlights = [[highlight] for highlight in batch[\"highlights\"]]\n",
    "    results = rouge.compute(\n",
    "        predictions=summaries,\n",
    "        references=highlights,\n",
    "        rouge_types=[\"rouge1\"],\n",
    "    )\n",
    "    batch_rouge_scores.append(results[\"rouge1\"])\n",
    "# Average all the ROUGE-1 scores together for the final one\n",
    "print(f\"Final Rouge Score: {sum(batch_rouge_scores)/len(batch_rouge_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the second prompt structure\n",
    "\n",
    "(text) \n",
    "\n",
    "Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch: 1\n",
      "Processing Batch: 2\n",
      "Processing Batch: 3\n",
      "Processing Batch: 4\n",
      "Processing Batch: 5\n",
      "Processing Batch: 6\n",
      "Processing Batch: 7\n",
      "Processing Batch: 8\n",
      "Processing Batch: 9\n",
      "Processing Batch: 10\n",
      "Final Rouge Score: 0.29087436021824853\n"
     ]
    }
   ],
   "source": [
    "max_batches = 10\n",
    "batch_rouge_scores = []\n",
    "for batch_number, batch in enumerate(dataloader, 1):\n",
    "    if batch_number > max_batches:\n",
    "        break\n",
    "    print(f\"Processing Batch: {batch_number}\")\n",
    "    truncated_articles = [truncate_article_text(text, falcon_tokenizer) for text in batch[\"article\"]]\n",
    "    prompts = [f\"{article_text}\\n{prompt_template_summary_2}\" for article_text in truncated_articles]\n",
    "\n",
    "    summaries = []\n",
    "    for p in prompts:\n",
    "        summaries.append(model.generate(p, long_generation_config).generation[\"sequences\"][0])\n",
    "\n",
    "    # Let's just take the first 3 sentences, split by periods\n",
    "    summaries = [post_process_generations(summary) for summary in summaries]\n",
    "    # References for the metric need to be in the form of list of lists\n",
    "    # (ROUGE can admit multiple references per prediction)\n",
    "    highlights = [[highlight] for highlight in batch[\"highlights\"]]\n",
    "    results = rouge.compute(\n",
    "        predictions=summaries,\n",
    "        references=highlights,\n",
    "        rouge_types=[\"rouge1\"],\n",
    "    )\n",
    "    batch_rouge_scores.append(results[\"rouge1\"])\n",
    "# Average all the ROUGE-1 scores together for the final one\n",
    "print(f\"Final Rouge Score: {sum(batch_rouge_scores)/len(batch_rouge_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second prompt, as measured by ROUGE-1 scores, appears to produce summaries of higher quality than the first prompt. This is potentially due to the way it is structured. Rather than asking a point blank question, we have the model fill in it's response in a structure way. Regardless, different prompts are likely to yield varying quality measures with respect to summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
