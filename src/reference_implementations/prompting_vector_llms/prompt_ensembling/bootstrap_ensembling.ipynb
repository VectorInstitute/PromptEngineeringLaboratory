{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from random import choice, sample\n",
    "from typing import List\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "from evaluate import EvaluationModule\n",
    "from tqdm import tqdm\n",
    "from utils import (\n",
    "    copa_preprocessor,\n",
    "    create_first_prompt,\n",
    "    create_first_prompt_label,\n",
    "    create_mc_prompt,\n",
    "    create_mc_prompt_answer,\n",
    "    split_prompts_into_batches,\n",
    ")\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a client connection to the Kaleidoscope service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'efd66405-d0ef-48d0-8ca0-2eb0c7c8127f',\n",
       "  'name': 'falcon-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'e64b3690-96c3-4a50-b95f-adcb85e2a42c',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll actually use two models, The first will be LLaMA-2 7B and the second will be Falcon 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while llama_model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_model = client.load_model(\"falcon-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while falcon_model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Ensembling for CoPA\n",
    "\n",
    "In the [CoPA Prompt Examples](copa_prompting_examples.ipynb) notebook, we looked at a few different ways to get the LLMs to perform the CoPA task. The best way was to evaluate the log probabilities of the candidates as completions, but we also considered a purely generative approach and a multiple choice forumulation. In this notebook, we'll see if we might be able to improve the results of the latter two methods with \"bootstrap\" ensembling.\n",
    "\n",
    "For a discussion of the CoPA task and the different prompting approach results, see the aforementioned notebook.\n",
    "\n",
    "We'll term the ensembling technique here \"bootstrap\" ensembling since we're not going to be using different prompt structures, just different demonstrations or generations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Choice Formulation\n",
    "\n",
    "We'll use the LLaMA-2 model here, as it seemed to perform better for the MC formulation. We're going to use the same prompt structure, but we're going to generate 7 distinct sets of demonstrations and use the models answers to vote on the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mc_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\"(A|B)\", single_generation)\n",
    "        # If you find an A or B in the answer use the first occurence. Otherwise randomly select one\n",
    "        if len(generation_text) == 0:\n",
    "            print(f\"Selecting Randomly. No selection match was found in: {single_generation}\")\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else choice([\"A\", \"B\"])\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:23<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MC Choice Response Number: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:24<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 8\n",
    "\n",
    "copa_data_set = copa_preprocessor(\"resources/copa_sample.tsv\")\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "\n",
    "# Note that we use a GREEDY decoding strategies for the model.\n",
    "llama_generation_config = {\"max_tokens\": 4, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "\n",
    "all_llama_responses = []\n",
    "\n",
    "number_of_voters = 7\n",
    "for voter_number in range(number_of_voters):\n",
    "    print(f\"Starting MC Choice Response Number: {voter_number + 1}\")\n",
    "    demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "    demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    int_labels = []\n",
    "    choices = []\n",
    "    for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "        int_labels.append(label)\n",
    "        choices.append((first_choice, second_choice))\n",
    "        labels.append(create_mc_prompt_answer(label))\n",
    "        prompts.append(create_mc_prompt(demonstrations, premise, phrase, first_choice, second_choice))\n",
    "\n",
    "    llama_responses = []\n",
    "    prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "    for prompt_batch in tqdm(prompt_batches):\n",
    "        generations = llama_model.generate(prompt_batch, llama_generation_config)\n",
    "        llama_responses.extend(process_mc_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "    all_llama_responses.append(llama_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for response_tuple, label in zip(zip(*all_llama_responses), labels):\n",
    "    predicted_label = Counter(response_tuple).most_common(1)[0][0]\n",
    "    total += 1\n",
    "    if predicted_label == label:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually get a pretty nice increase in the accuracy here (it was only 0.68 in the other notebook). The quality and distribution of the demonstrations has an impact on how well a model performs a task. By sampling several sets of demonstrations, we were able to improve our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Formulation\n",
    "\n",
    "We'll use Falcon for this example as an alternative to using LLaMA-2 above. It performed a little better in the previous notebook example as well. In this setting, we're going to use the same set of demonstrations, but we'll allow the model to sample different generation trajectories and see what happens when we use a simple voting strategy to tabulate the prediction. Generation is a bit more \"expensive\" than the multiple choice setting. So we'll perform 3 distinct generations and see if we can get any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generative Response Number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:10<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generative Response Number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:05<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generative Response Number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:59<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "\n",
    "# Note that we use a non-greedy decoding strategies for this model to produce variation in the responses.\n",
    "falcon_generation_config = {\"max_tokens\": 20, \"top_k\": 4, \"temperature\": 0.8, \"do_sample\": True}\n",
    "\n",
    "all_falcon_responses = []\n",
    "\n",
    "number_of_voters = 3\n",
    "for voter_number in range(number_of_voters):\n",
    "    print(f\"Starting Generative Response Number: {voter_number + 1}\")\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    int_labels = []\n",
    "    choices = []\n",
    "    for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "        int_labels.append(label)\n",
    "        choices.append((first_choice, second_choice))\n",
    "        labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "        prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))\n",
    "\n",
    "    falcon_responses = []\n",
    "    prompt_batches = split_prompts_into_batches(prompts, 1)\n",
    "    for prompt_batch in tqdm(prompt_batches):\n",
    "        generations = falcon_model.generate(prompt_batch, falcon_generation_config)\n",
    "        falcon_responses.extend(process_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "    all_falcon_responses.append(falcon_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_rouge(\n",
    "    response: str, first_choice: str, second_choice: str, rouge_metric: EvaluationModule\n",
    ") -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    # Use the rouge metric to score the response against the first choice or second choice as reference\n",
    "    rouge_0 = rouge_metric.compute(predictions=[response], references=[first_choice])\n",
    "    rouge_1 = rouge_metric.compute(predictions=[response], references=[second_choice])\n",
    "    # We take the average of the unigram and bi-gram rouge scores for the first and second choice results.\n",
    "    score_0 = (rouge_0[\"rouge1\"] + rouge_0[\"rouge2\"]) / 2.0\n",
    "    score_1 = (rouge_1[\"rouge1\"] + rouge_1[\"rouge2\"]) / 2.0\n",
    "    # If the first score is larger we select the first choice\n",
    "    return 0 if score_0 > score_1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 Response Tuples\n",
      "Processed 20 Response Tuples\n",
      "Processed 30 Response Tuples\n",
      "Processed 40 Response Tuples\n",
      "Processed 50 Response Tuples\n",
      "Processed 60 Response Tuples\n",
      "Processed 70 Response Tuples\n",
      "Processed 80 Response Tuples\n",
      "Processed 90 Response Tuples\n",
      "Processed 100 Response Tuples\n",
      "Falcon Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for falcon_response_tuple, label_int, (first_choice, second_choice) in zip(\n",
    "    zip(*all_falcon_responses), int_labels, choices\n",
    "):\n",
    "    predicted_labels = []\n",
    "    for response in falcon_response_tuple:\n",
    "        predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "        predicted_labels.append(predicted_label)\n",
    "    majority_prediction = Counter(predicted_labels).most_common(1)[0][0]\n",
    "    total += 1\n",
    "    if majority_prediction == label_int:\n",
    "        correct += 1\n",
    "    if total % 10 == 0:\n",
    "        print(f\"Processed {total} Response Tuples\")\n",
    "\n",
    "print(f\"Falcon Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our stochastic generation didn't help us improve accuracy in this case. In the previous notebook, we saw an accuracy of 0.68. However, we only produced three families of generations. So it's possible that running additional generations would help. While we didn't do so here, we might also have tried to resample the demonstration examples along with stochastically generating the responses and seen if that improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
