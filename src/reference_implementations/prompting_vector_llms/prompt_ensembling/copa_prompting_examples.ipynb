{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from random import choice, sample\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import kscope\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from evaluate import EvaluationModule\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from utils import (\n",
    "    copa_preprocessor,\n",
    "    create_first_prompt,\n",
    "    create_first_prompt_label,\n",
    "    create_mc_prompt,\n",
    "    create_mc_prompt_answer,\n",
    "    create_second_prompt,\n",
    "    create_second_prompt_label,\n",
    "    split_prompts_into_batches,\n",
    ")\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "random.seed(2024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a client connection to the Kaleidoscope service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'f0973d9f-d6d7-41ab-974c-e398ed1abc21',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '38c590c0-f3b3-4331-8296-74e6068a7106',\n",
       "  'name': 'falcon-7b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll perform some experimentation with both the LLaMA-2 and Falcon 7B parameter models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while llama_model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_model = client.load_model(\"falcon-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while falcon_model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Target Task: Balanced Choice of Plausible Alternatives (CoPA)\n",
    "\n",
    "We'll use an updated (harder) version of the CoPA dataset. We're only going to work with a small subset of the true development set in order to expedite LLM evaluation. \n",
    "\n",
    "The task, in short, is, given a context and a premise of either cause or effect, the model must choose between two distinct sentences to determine which is the logical following sentence. \n",
    "\n",
    "Two examples are:\n",
    "\n",
    "From the following choices,\n",
    "1) The author faded into obscurity.\n",
    "2) It was adapted into a movie.\n",
    "\n",
    "and an __effect__ premise, which logically follows the sentence \"The book became a huge failure.\" The answer is \"The author faded into obscurity.\" \n",
    "\n",
    "From the following choices,\n",
    "1) The shop was undergoing renovation.\n",
    "2) The owner was helping customers.\n",
    "\n",
    "and a __cause__ premise, which logically follows the sentence \"The shop was closed.\" The answer is \"The shop was undergoing renovation.\" \n",
    "\n",
    "You can inspect the preprocessed dataset at \n",
    "\n",
    "`src/reference_implementations/prompting_vector_llms/prompt_ensembling/resources/copa_sample.tsv`\n",
    "\n",
    "We print out some of the demonstrations that we've setup below for additional reference.\n",
    "\n",
    "__NOTE__: Construction of the prompts and some other functions that are used throughout this notebook have been pulled into a utils file \n",
    "\n",
    "`src/reference_implementations/prompting_vector_llms/prompt_ensembling/utils.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Generation Formulation\n",
    "\n",
    "For our first approach to this task, we'll attempt to have the model generate exact matches to the possible choices. We expect this to be somewhat brittle, as it relies on the model producing an a copy (or something \"nearby\") of the selected completion.\n",
    "\n",
    "__NOTE__: In general, when we see an \"effect\" premise the string \", so\" is added to the phrase to be completed. If the premise is \"cause\" then then string \", because\" is added to the phrase to be completed. We strip out the ending period to improve fluency. See the demonstrations below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the initial data points should be reserved for demonstrations\n",
    "demonstration_candidates = 50\n",
    "# Number of demonstrations to be used per prompt\n",
    "n_demonstrations = 10\n",
    "\n",
    "copa_data_set = copa_preprocessor(\"resources/copa_sample.tsv\")\n",
    "\n",
    "demonstration_pool = copa_data_set[0:demonstration_candidates]\n",
    "test_pool = copa_data_set[demonstration_candidates:]\n",
    "demonstrations = sample(demonstration_pool, n_demonstrations)\n",
    "prompts: List[str] = []\n",
    "labels: List[str] = []\n",
    "int_labels: List[int] = []\n",
    "choices: List[Tuple[str, str]] = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_first_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_first_prompt(demonstrations, premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the sentence that best completes the phrase\n",
      "\n",
      "\"the student's phone rang.\" or \"the student took notes.\"\n",
      "Everyone in the class turned to stare at the student, because the student's phone rang.\n",
      "\n",
      "\"the teapot whistled.\" or \"the teapot cooled.\"\n",
      "The water in the teapot started to boil, so the teapot whistled.\n",
      "\n",
      "\"the girl applied her makeup.\" or \"the girl turned on the fan.\"\n",
      "The mirror in the bathroom fogged up, so the girl turned on the fan.\n",
      "\n",
      "\"the surveillance camera was out of focus.\" or \"he noticed some suspicious activity.\"\n",
      "The security guard could not identify the thief, because the surveillance camera was out of focus.\n",
      "\n",
      "\"he trusted the therapist.\" or \"he disagreed with the therapist.\"\n",
      "The man revealed personal information to the therapist, because he trusted the therapist.\n",
      "\n",
      "\"she contacted her lawyer.\" or \"she cancelled her appointments.\"\n",
      "The woman was summoned for jury duty, so she cancelled her appointments.\n",
      "\n",
      "\"the student's phone rang.\" or \"the student took notes.\"\n",
      "The teacher covered a lot of material, because the student took notes.\n",
      "\n",
      "\"the motorcyclist died.\" or \"the bridge collapsed.\"\n",
      "The truck crashed into the motorcycle on the bridge, so the motorcyclist died.\n",
      "\n",
      "\"the pants were new.\" or \"the pocket had a hole.\"\n",
      "The pants had no defects, because the pants were new.\n",
      "\n",
      "\"i was preparing to clean the bathroom.\" or \"i was preparing to wash my hands.\"\n",
      "I took off the rubber gloves, because i was preparing to wash my hands.\n",
      "\n",
      "\"They rested.\" or \"They kissed.\"\n",
      "The couple was very tired, so  \n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this prompt performs on a small sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\".*?[.!\\?]\", single_generation)\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else single_generation\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that both of these configuration specify GREEDY decoding strategies for the different models\n",
    "llama_generation_config = {\"max_tokens\": 35, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "falcon_generation_config = {\"max_tokens\": 35, \"top_k\": 1, \"temperature\": 1.0, \"do_sample\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_to_run = 3\n",
    "llama_generations = llama_model.generate(prompts[0:n_samples_to_run], llama_generation_config)\n",
    "llama_responses = process_generation_text(llama_generations.generation[\"sequences\"])\n",
    "\n",
    "falcon_generations = falcon_model.generate(prompts[0:n_samples_to_run], falcon_generation_config)\n",
    "falcon_responses = process_generation_text(falcon_generations.generation[\"sequences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Response: \"the student's phone rang.\n",
      "Label: they rested.\n",
      "\n",
      "LLaMA Response: I stopped receiving new issues.\n",
      "Label: i discarded the new issue.\n",
      "\n",
      "LLaMA Response: She felt self-conscious.\n",
      "Label: she felt self-conscious.\n",
      "\n",
      "Falcon Response:  they rested.\n",
      "Label: they rested.\n",
      "\n",
      "Falcon Response:  I discarded the new issue.\n",
      "Label: i discarded the new issue.\n",
      "\n",
      "Falcon Response:  she felt self-conscious.\n",
      "Label: she felt self-conscious.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for response, label in zip(llama_responses, labels[0:n_samples_to_run]):\n",
    "    print(f\"LLaMA Response: {response}\\nLabel: {label}\\n\")\n",
    "\n",
    "for response, label in zip(falcon_responses, labels[0:n_samples_to_run]):\n",
    "    print(f\"Falcon Response: {response}\\nLabel: {label}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring generated Responses for the 10-shot Prompt Above\n",
    "\n",
    "Here we consider the performance of the demonstration prompt above on our subsampling of the CoPA dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all of the examples through the models and collect the responses into responses lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:44<00:00,  4.46s/it]\n",
      "100%|██████████| 13/13 [04:24<00:00, 20.31s/it]\n"
     ]
    }
   ],
   "source": [
    "llama_responses = []\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generations = llama_model.generate(prompt_batch, llama_generation_config)\n",
    "    llama_responses.extend(process_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "falcon_responses = []\n",
    "# Falcon requires a batch size of 8 or less\n",
    "prompt_batches = split_prompts_into_batches(prompts, 8)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generations = falcon_model.generate(prompt_batch, falcon_generation_config)\n",
    "    falcon_responses.extend(process_generation_text(generations.generation[\"sequences\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform scoring based on the generated text, by considering the rouge score of the responses using the label as the reference. We choose between the two available choices for the logical completion of the reference phrase. The model has provided a response and we treat each choice as a reference for the ROUGE metric. We take as the model's prediction the phrase with the highest ROUGE score compared to the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response_via_rouge(\n",
    "    response: str, first_choice: str, second_choice: str, rouge_metric: EvaluationModule\n",
    ") -> int:\n",
    "    response = response.lower()\n",
    "    first_choice = first_choice.lower()\n",
    "    second_choice = second_choice.lower()\n",
    "    # Use the rouge metric to score the response against the first choice or second choice as reference\n",
    "    rouge_0 = rouge_metric.compute(predictions=[response], references=[first_choice])\n",
    "    rouge_1 = rouge_metric.compute(predictions=[response], references=[second_choice])\n",
    "    # We take the average of the unigram and bi-gram rouge scores for the first and second choice results.\n",
    "    score_0 = (rouge_0[\"rouge1\"] + rouge_0[\"rouge2\"]) / 2.0\n",
    "    score_1 = (rouge_1[\"rouge1\"] + rouge_1[\"rouge2\"]) / 2.0\n",
    "    # If the first score is larger we select the first choice\n",
    "    return 0 if score_0 > score_1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Accuracy: 0.66\n",
      "Falcon Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for response, label_int, (first_choice, second_choice) in zip(llama_responses, int_labels, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"LLaMA Accuracy: {correct/total}\")\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for response, label_int, (first_choice, second_choice) in zip(falcon_responses, int_labels, choices):\n",
    "    predicted_label = score_response_via_rouge(response, first_choice, second_choice, rouge_metric)\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Falcon Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We score above random chance for this problem, but we'd certainly like to do better. We'll try a few additional formulations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Multiple Choice Formulation\n",
    "\n",
    "Instead of formulating the task as a completion, we can try using a multiple choice type construction and see how well the model does the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_mc_prompt_answer(label))\n",
    "    prompts.append(create_mc_prompt(demonstrations[2:], premise, phrase, first_choice, second_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The mirror in the bathroom fogged up, so\n",
      "A: the girl applied her makeup.\n",
      "B: the girl turned on the fan.\n",
      "Answer: B\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The security guard could not identify the thief, because\n",
      "A: the surveillance camera was out of focus.\n",
      "B: he noticed some suspicious activity.\n",
      "Answer: A\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The man revealed personal information to the therapist, because\n",
      "A: he trusted the therapist.\n",
      "B: he disagreed with the therapist.\n",
      "Answer: A\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The woman was summoned for jury duty, so\n",
      "A: she contacted her lawyer.\n",
      "B: she cancelled her appointments.\n",
      "Answer: B\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The teacher covered a lot of material, because\n",
      "A: the student's phone rang.\n",
      "B: the student took notes.\n",
      "Answer: B\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The truck crashed into the motorcycle on the bridge, so\n",
      "A: the motorcyclist died.\n",
      "B: the bridge collapsed.\n",
      "Answer: A\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The pants had no defects, because\n",
      "A: the pants were new.\n",
      "B: the pocket had a hole.\n",
      "Answer: A\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: I took off the rubber gloves, because\n",
      "A: i was preparing to clean the bathroom.\n",
      "B: i was preparing to wash my hands.\n",
      "Answer: B\n",
      "\n",
      "From A or B which choice best completes the phrase?\n",
      "Phrase: The couple was very tired, so\n",
      "A: they rested.\n",
      "B: they kissed.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that both of these are GREEDY decoding strategies for the different models.\n",
    "# Since we're generating a multiple choice answer, we shorten the max tokens.\n",
    "llama_generation_config = {\"max_tokens\": 4, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "falcon_generation_config = {\"max_tokens\": 4, \"top_k\": 1, \"temperature\": 1.0, \"do_sample\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mc_generation_text(original_texts: List[str]) -> List[str]:\n",
    "    responses = []\n",
    "    for single_generation in original_texts:\n",
    "        generation_text: List[str] = re.findall(r\"(A|B)\", single_generation)\n",
    "        # If you find an A or B in the answer use the first occurence. Otherwise randomly select one\n",
    "        if len(generation_text) == 0:\n",
    "            print(f\"Selecting Randomly. No selection match was found in: {single_generation}\")\n",
    "        response_text = generation_text[0] if len(generation_text) > 0 else choice([\"A\", \"B\"])\n",
    "        responses.append(response_text)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_to_run = 3\n",
    "llama_generations = llama_model.generate(prompts[0:n_samples_to_run], llama_generation_config)\n",
    "llama_responses = process_mc_generation_text(llama_generations.generation[\"sequences\"])\n",
    "\n",
    "falcon_generations = falcon_model.generate(prompts[0:n_samples_to_run], falcon_generation_config)\n",
    "falcon_responses = process_mc_generation_text(falcon_generations.generation[\"sequences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Response: A\n",
      "Label: A\n",
      "\n",
      "LLaMA Response: A\n",
      "Label: B\n",
      "\n",
      "LLaMA Response: A\n",
      "Label: A\n",
      "\n",
      "Falcon Response: B\n",
      "Label: A\n",
      "\n",
      "Falcon Response: A\n",
      "Label: B\n",
      "\n",
      "Falcon Response: A\n",
      "Label: A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for response, label in zip(llama_responses, labels[0:n_samples_to_run]):\n",
    "    print(f\"LLaMA Response: {response}\\nLabel: {label}\\n\")\n",
    "\n",
    "for response, label in zip(falcon_responses, labels[0:n_samples_to_run]):\n",
    "    print(f\"Falcon Response: {response}\\nLabel: {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.53s/it]\n",
      "100%|██████████| 13/13 [00:38<00:00,  2.94s/it]\n"
     ]
    }
   ],
   "source": [
    "llama_responses = []\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generations = llama_model.generate(prompt_batch, llama_generation_config)\n",
    "    llama_responses.extend(process_mc_generation_text(generations.generation[\"sequences\"]))\n",
    "\n",
    "falcon_responses = []\n",
    "# Falcon requires a batch size of 8 or less\n",
    "prompt_batches = split_prompts_into_batches(prompts, 8)\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generations = falcon_model.generate(prompt_batch, falcon_generation_config)\n",
    "    falcon_responses.extend(process_mc_generation_text(generations.generation[\"sequences\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Accuracy: 0.68\n",
      "Falcon Accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label_str, label in zip(llama_responses, labels):\n",
    "    if predicted_label_str == label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"LLaMA Accuracy: {correct/total}\")\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label_str, label in zip(falcon_responses, labels):\n",
    "    if predicted_label_str == label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Falcon Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, the LLaMA model performs significantly better than Falcon. However, we're still not doing the task with as high accuracy as we'd like to. In the [Bootstrap Ensembling Notebook](bootstrap_ensembling.ipynb), we'll consider ways to improve this formulation through ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Log-Likelihood Formulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, for each prompt and the two responses, we can score the candidate responses by log likelihood (from the models' perspective) and choose the higher one as our label. That is, we complete the prompt with both labels and then extract the log-likelihoods of that input text from the perspective of the model. See the comments in the code below for more details on how this is done.\n",
    "\n",
    "__NOTE__: In our current implementations, __only LLaMA-2__ is configured to produce activations. So we'll only use that model in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer \n",
    "\n",
    "For activation retrieval, we need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All LLaMA-2 models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed.\n",
    "\n",
    "If you are on the cluster, the tokenizer may be loaded from `/model-weights/Llama-2-7b-hf`. Otherwise, you'll need to download the `config.json`, `tokenizer.json`, `tokenizer.model`, and `tokenizer_config.json` from there to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [1, 15043, 445, 338, 263, 1243]\n",
      "Decoded Tokens: <s> Hello this is a test\n",
      "Last Layer Name: output\n",
      "Endline Token Id: 13\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer prepares the input of the model. LLaMA models of all sizes use the same underlying tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/model-weights/Llama-2-7b-hf\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"BOS\")\n",
    "encoded_tokens = tokenizer.encode(\"Hello this is a test\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "decoded_tokens = tokenizer.decode(encoded_tokens)\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")\n",
    "\n",
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods\n",
    "last_layer_name = llama_model.module_names[-1]\n",
    "print(f\"Last Layer Name: {last_layer_name}\")\n",
    "# Get a log softmax function to compute log probabilities from the output layer.\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "endline_token_id = tokenizer.encode(\"Hello\\n\")[-1]\n",
    "print(f\"Endline Token Id: {endline_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probability_from_activations(logits: torch.Tensor, token_ids: List[int]) -> float:\n",
    "    # First we get the logprobs associated with each token, logits is n_tokens x vocabulary size\n",
    "    log_probs = log_softmax(logits.type(torch.float32))\n",
    "    # Drop the first token ID (as it corresponds to the <s> token) and add placeholder to the end. This shift aligns\n",
    "    # The tokens with the output activations corresponding to their logprobs\n",
    "    token_ids.pop(0)\n",
    "    token_ids.append(1)\n",
    "    # We only really care about the logprobs associated with the sentence to be completed\n",
    "    # (i.e. not the demonstrations or the question). So search for the last endline in the tokens and only\n",
    "    # sum the logprobs thereafter.\n",
    "    endline_index = len(token_ids) - list(reversed(token_ids)).index(endline_token_id)\n",
    "    # Turn token ids into the appropriate column indices\n",
    "    token_id_slicer = torch.Tensor(token_ids).reshape(-1, 1).type(torch.int64)\n",
    "    log_probs_per_token = log_probs.gather(1, token_id_slicer)\n",
    "    # We sum the log probabilities, except for the last one which corresponds to the as yet predicted token)\n",
    "    # and then normalize by the number of tokens (minus one for the placeholder)\n",
    "    selected_log_probs_per_token = log_probs_per_token[endline_index:-1]\n",
    "    normalized_log_prob = torch.sum(selected_log_probs_per_token) / len(selected_log_probs_per_token)\n",
    "    return normalized_log_prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're running a lot of activation retrievals. Once in a while there is a json decoding or Triton error. If that\n",
    "# happens, we retry the activations request.\n",
    "def get_activations_with_retries(prompt: str, layers: List[str], config: Dict[str, Any], retries: int = 5) -> Any:\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            return llama_model.get_activations(prompt, layers, config)\n",
    "        except Exception as e:  # noqa: F841\n",
    "            print(\"Something went wrong in activation retrieval...retrying\")\n",
    "    raise ValueError(\"Exceeded retry limit. Exiting Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_prompts_with_choices(prompt_batch: List[Tuple[str, Tuple[str, str]]]) -> List[str]:\n",
    "    # We want to complete our prompt with the two possible choices and score those completions using our LM.\n",
    "    prompts_with_choices = []\n",
    "    for prompt, (first_choice, second_choice) in prompt_batch:\n",
    "        prompts_with_choices.append(f\"{prompt}{first_choice.lower()}\")\n",
    "        prompts_with_choices.append(f\"{prompt}{second_choice.lower()}\")\n",
    "    return prompts_with_choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a log likelihood for each prompt completion corresponding to completion with the first or second potential phrase, we pair those up and compute which has the higher likelihood between the two options. This then becomes our \"predicted\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_logprobs_to_labels(logprobs: List[float]) -> Tuple[List[int], List[List[float]]]:\n",
    "    # Need to group logprobs in twos because they represent likelihoods of the two completions\n",
    "    assert len(logprobs) % 2 == 0\n",
    "    paired_logprobs = [logprobs[x : x + 2] for x in range(0, len(logprobs), 2)]\n",
    "    predicted_labels: List[int] = []\n",
    "    predicted_logprobs = []\n",
    "    for logprob_pair in paired_logprobs:\n",
    "        # Paired logprob for first and second choice together\n",
    "        predicted_labels.append(np.argmax(logprob_pair, axis=0))\n",
    "        predicted_logprobs.append(logprob_pair)\n",
    "    return predicted_labels, predicted_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "labels = []\n",
    "int_labels = []\n",
    "choices = []\n",
    "for premise, label, phrase, first_choice, second_choice in test_pool:\n",
    "    int_labels.append(label)\n",
    "    choices.append((first_choice, second_choice))\n",
    "    labels.append(create_second_prompt_label(first_choice.lower(), second_choice.lower(), label))\n",
    "    prompts.append(create_second_prompt(demonstrations, premise, phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete the phrase with a logical phrase.\n",
      "\n",
      "Everyone in the class turned to stare at the student, because the student's phone rang.\n",
      "\n",
      "The water in the teapot started to boil, so the teapot whistled.\n",
      "\n",
      "The mirror in the bathroom fogged up, so the girl turned on the fan.\n",
      "\n",
      "The security guard could not identify the thief, because the surveillance camera was out of focus.\n",
      "\n",
      "The man revealed personal information to the therapist, because he trusted the therapist.\n",
      "\n",
      "The woman was summoned for jury duty, so she cancelled her appointments.\n",
      "\n",
      "The teacher covered a lot of material, because the student took notes.\n",
      "\n",
      "The truck crashed into the motorcycle on the bridge, so the motorcyclist died.\n",
      "\n",
      "The pants had no defects, because the pants were new.\n",
      "\n",
      "I took off the rubber gloves, because i was preparing to wash my hands.\n",
      "\n",
      "The couple was very tired, so \n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [15:28<00:00,  4.64s/it]\n"
     ]
    }
   ],
   "source": [
    "all_logprobs = []\n",
    "prompts_and_choices = pair_prompts_with_choices(list(zip(prompts, choices)))\n",
    "# prompts and choices is now twice as long as the original prompts and choices because the prompts have been completed\n",
    "# with the two possible choices\n",
    "# We split the prompts into batches of 1 for memory management since activation retrieval is a bit heavy.\n",
    "prompt_batches = split_prompts_into_batches(prompts_and_choices, 1)\n",
    "llama_generation_config = {\"max_tokens\": 1, \"top_p\": 1.0, \"temperature\": 0.0}\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    # Process below only works for batches of size 1\n",
    "    assert len(prompt_batch) == 1\n",
    "    single_prompt = prompt_batch[0]\n",
    "    # The score for a sentence is the sum of log probability of each word in the sentence.\n",
    "    prompt_activations = get_activations_with_retries(single_prompt, [last_layer_name], llama_generation_config)  # type: ignore # noqa: E501\n",
    "    token_ids = tokenizer.encode(single_prompt)\n",
    "    last_layer_matrix = prompt_activations.activations[0][last_layer_name]\n",
    "    prompt_log_probs = compute_log_probability_from_activations(last_layer_matrix, token_ids)\n",
    "    all_logprobs.append(prompt_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "predicted_labels, _ = post_process_logprobs_to_labels(all_logprobs)\n",
    "total = 0\n",
    "correct = 0\n",
    "for predicted_label, label_int in zip(predicted_labels, int_labels):\n",
    "    if predicted_label == label_int:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice boost to the accuracy that we've been seeing with our other formulations. In some follow up notebooks, we'll see if we can improve up the results of this notebook with a few basic ensembling techniques. These notebooks are [Bootstrap Ensembling](bootstrap_ensembling.ipynb) and [Prompt Ensembling](prompt_ensembling.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
